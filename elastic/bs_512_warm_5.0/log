INFO:root:Namespace(batch_size=128, drop_rate=0.0, gpus='0,1,2,3', lr=0.4, lr_decay=0.1, lr_decay_epoch='100,150', mode='hybrid', model='cifar_resnet20_v1', momentum=0.9, num_epochs=200, num_workers=2, resume_from=None, save_dir='bs_512_warm_5.0', save_period=10, save_plot_dir='bs_512_warm_5.0', warmup_epochs=5, wd=0.0001)
[02:39:37] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:[Epoch 0] train=0.341555 val=0.424100 loss=1.787903 time: 17.258940 lr: 0.079175
INFO:root:[Epoch 1] train=0.570615 val=0.565500 loss=1.188702 time: 15.509004 lr: 0.159175
INFO:root:[Epoch 2] train=0.673989 val=0.676700 loss=0.914558 time: 15.004233 lr: 0.239175
INFO:root:[Epoch 3] train=0.734154 val=0.701100 loss=0.758203 time: 14.745847 lr: 0.319175
INFO:root:[Epoch 4] train=0.769008 val=0.686800 loss=0.661726 time: 14.587364 lr: 0.399175
INFO:root:[Epoch 5] train=0.791982 val=0.707600 loss=0.603067 time: 14.887015 lr: 0.400000
INFO:root:[Epoch 6] train=0.813426 val=0.765400 loss=0.536644 time: 14.850653 lr: 0.400000
INFO:root:[Epoch 7] train=0.826877 val=0.786800 loss=0.499632 time: 14.538156 lr: 0.400000
INFO:root:[Epoch 8] train=0.836421 val=0.775000 loss=0.470912 time: 14.945510 lr: 0.400000
INFO:root:[Epoch 9] train=0.843911 val=0.777100 loss=0.451351 time: 15.027471 lr: 0.400000
INFO:root:[Epoch 10] train=0.851905 val=0.798600 loss=0.427324 time: 14.860110 lr: 0.400000
INFO:root:[Epoch 11] train=0.859496 val=0.759000 loss=0.408340 time: 15.886722 lr: 0.400000
INFO:root:[Epoch 12] train=0.862496 val=0.792800 loss=0.395930 time: 16.114214 lr: 0.400000
INFO:root:[Epoch 13] train=0.868516 val=0.778000 loss=0.383000 time: 15.003238 lr: 0.400000
INFO:root:[Epoch 14] train=0.871879 val=0.821200 loss=0.367741 time: 15.759193 lr: 0.400000
INFO:root:[Epoch 15] train=0.876228 val=0.822800 loss=0.361140 time: 15.677385 lr: 0.400000
INFO:root:[Epoch 16] train=0.877215 val=0.805300 loss=0.352186 time: 15.573255 lr: 0.400000
INFO:root:[Epoch 17] train=0.883598 val=0.825900 loss=0.340615 time: 16.091768 lr: 0.400000
INFO:root:[Epoch 18] train=0.883739 val=0.836000 loss=0.336994 time: 15.812290 lr: 0.400000
INFO:root:[Epoch 19] train=0.886397 val=0.818100 loss=0.325048 time: 15.717107 lr: 0.400000
INFO:root:[Epoch 20] train=0.887927 val=0.821300 loss=0.325183 time: 15.723544 lr: 0.400000
INFO:root:[Epoch 21] train=0.888249 val=0.841700 loss=0.320612 time: 15.230373 lr: 0.400000
INFO:root:[Epoch 22] train=0.891531 val=0.852500 loss=0.311260 time: 15.988491 lr: 0.400000
INFO:root:[Epoch 23] train=0.895196 val=0.835500 loss=0.303187 time: 15.704788 lr: 0.400000
INFO:root:[Epoch 24] train=0.897028 val=0.795400 loss=0.297747 time: 15.874262 lr: 0.400000
INFO:root:[Epoch 25] train=0.898498 val=0.821900 loss=0.291858 time: 16.143755 lr: 0.400000
INFO:root:[Epoch 26] train=0.901861 val=0.811500 loss=0.285427 time: 15.567925 lr: 0.400000
INFO:root:[Epoch 27] train=0.900753 val=0.846200 loss=0.285650 time: 15.782551 lr: 0.400000
INFO:root:[Epoch 28] train=0.901035 val=0.787000 loss=0.281259 time: 15.807841 lr: 0.400000
INFO:root:[Epoch 29] train=0.902746 val=0.852800 loss=0.277160 time: 16.184001 lr: 0.400000
INFO:root:[Epoch 30] train=0.903653 val=0.831700 loss=0.276366 time: 16.437101 lr: 0.400000
INFO:root:[Epoch 31] train=0.904297 val=0.829000 loss=0.270838 time: 16.133749 lr: 0.400000
INFO:root:[Epoch 32] train=0.907559 val=0.833700 loss=0.264048 time: 15.832891 lr: 0.400000
INFO:root:[Epoch 33] train=0.904478 val=0.835300 loss=0.270937 time: 16.120582 lr: 0.400000
INFO:root:[Epoch 34] train=0.909633 val=0.850400 loss=0.257136 time: 16.076912 lr: 0.400000
INFO:root:[Epoch 35] train=0.908707 val=0.837600 loss=0.259983 time: 16.749771 lr: 0.400000
INFO:root:[Epoch 36] train=0.907760 val=0.833800 loss=0.260870 time: 16.826632 lr: 0.400000
INFO:root:[Epoch 37] train=0.912613 val=0.829000 loss=0.251561 time: 17.292286 lr: 0.400000
INFO:root:[Epoch 38] train=0.911284 val=0.815700 loss=0.254866 time: 16.886658 lr: 0.400000
INFO:root:[Epoch 39] train=0.912270 val=0.853700 loss=0.248124 time: 16.937981 lr: 0.400000
INFO:root:[Epoch 40] train=0.914687 val=0.842300 loss=0.244706 time: 16.966630 lr: 0.400000
INFO:root:[Epoch 41] train=0.913599 val=0.838700 loss=0.246882 time: 17.085181 lr: 0.400000
INFO:root:[Epoch 42] train=0.913760 val=0.800700 loss=0.247249 time: 17.076092 lr: 0.400000
INFO:root:[Epoch 43] train=0.916257 val=0.815600 loss=0.239455 time: 17.102588 lr: 0.400000
INFO:root:[Epoch 44] train=0.917204 val=0.844800 loss=0.239493 time: 17.093664 lr: 0.400000
INFO:root:[Epoch 45] train=0.915492 val=0.847900 loss=0.239050 time: 17.105758 lr: 0.400000
INFO:root:[Epoch 46] train=0.917465 val=0.815400 loss=0.237034 time: 16.983069 lr: 0.400000
INFO:root:[Epoch 47] train=0.916157 val=0.838900 loss=0.240565 time: 16.926486 lr: 0.400000
INFO:root:[Epoch 48] train=0.915774 val=0.818700 loss=0.241632 time: 17.274002 lr: 0.400000
INFO:root:[Epoch 49] train=0.918432 val=0.867000 loss=0.233941 time: 16.930663 lr: 0.400000
INFO:root:[Epoch 50] train=0.918110 val=0.856000 loss=0.231250 time: 17.061660 lr: 0.400000
INFO:root:[Epoch 51] train=0.918452 val=0.841000 loss=0.235409 time: 16.750649 lr: 0.400000
INFO:root:[Epoch 52] train=0.917828 val=0.837400 loss=0.233706 time: 17.492675 lr: 0.400000
INFO:root:[Epoch 53] train=0.918633 val=0.850200 loss=0.229684 time: 17.099137 lr: 0.400000
INFO:root:[Epoch 54] train=0.919942 val=0.821800 loss=0.226911 time: 17.126291 lr: 0.400000
INFO:root:[Epoch 55] train=0.920909 val=0.840900 loss=0.227968 time: 16.721160 lr: 0.400000
INFO:root:[Epoch 56] train=0.920768 val=0.849500 loss=0.223285 time: 16.742895 lr: 0.400000
INFO:root:[Epoch 57] train=0.920868 val=0.867600 loss=0.226925 time: 17.127811 lr: 0.400000
INFO:root:[Epoch 58] train=0.921573 val=0.850400 loss=0.223152 time: 17.247002 lr: 0.400000
INFO:root:[Epoch 59] train=0.920566 val=0.836800 loss=0.226676 time: 17.293783 lr: 0.400000
INFO:root:[Epoch 60] train=0.922902 val=0.851500 loss=0.219954 time: 16.575560 lr: 0.400000
INFO:root:[Epoch 61] train=0.922278 val=0.855300 loss=0.219130 time: 17.465661 lr: 0.400000
INFO:root:[Epoch 62] train=0.922761 val=0.851700 loss=0.221304 time: 16.957033 lr: 0.400000
INFO:root:[Epoch 63] train=0.922157 val=0.821400 loss=0.218876 time: 17.139688 lr: 0.400000
INFO:root:[Epoch 64] train=0.923123 val=0.842800 loss=0.216930 time: 16.869796 lr: 0.400000
INFO:root:[Epoch 65] train=0.922962 val=0.849400 loss=0.221178 time: 17.036985 lr: 0.400000
INFO:root:[Epoch 66] train=0.924070 val=0.836500 loss=0.215150 time: 17.099346 lr: 0.400000
INFO:root:[Epoch 67] train=0.924412 val=0.855600 loss=0.217275 time: 17.336529 lr: 0.400000
INFO:root:[Epoch 68] train=0.922982 val=0.809000 loss=0.217108 time: 16.751026 lr: 0.400000
INFO:root:[Epoch 69] train=0.926184 val=0.853500 loss=0.209822 time: 16.777413 lr: 0.400000
INFO:root:[Epoch 70] train=0.924996 val=0.866600 loss=0.216495 time: 16.996585 lr: 0.400000
INFO:root:[Epoch 71] train=0.926526 val=0.817900 loss=0.210220 time: 17.593416 lr: 0.400000
INFO:root:[Epoch 72] train=0.923506 val=0.844500 loss=0.213268 time: 16.879048 lr: 0.400000
INFO:root:[Epoch 73] train=0.926969 val=0.867200 loss=0.208683 time: 17.247926 lr: 0.400000
INFO:root:[Epoch 74] train=0.927110 val=0.860100 loss=0.209116 time: 17.146381 lr: 0.400000
INFO:root:[Epoch 75] train=0.926526 val=0.864400 loss=0.209415 time: 16.807592 lr: 0.400000
INFO:root:[Epoch 76] train=0.925338 val=0.830500 loss=0.210597 time: 17.115920 lr: 0.400000
INFO:root:[Epoch 77] train=0.925741 val=0.851100 loss=0.212806 time: 17.361744 lr: 0.400000
INFO:root:[Epoch 78] train=0.927895 val=0.869400 loss=0.206096 time: 17.345286 lr: 0.400000
INFO:root:[Epoch 79] train=0.926808 val=0.845200 loss=0.207245 time: 17.199051 lr: 0.400000
INFO:root:[Epoch 80] train=0.926808 val=0.833400 loss=0.210090 time: 17.156571 lr: 0.400000
INFO:root:[Epoch 81] train=0.925761 val=0.817200 loss=0.209564 time: 17.015267 lr: 0.400000
INFO:root:[Epoch 82] train=0.929567 val=0.851400 loss=0.201918 time: 17.181041 lr: 0.400000
INFO:root:[Epoch 83] train=0.927775 val=0.842800 loss=0.208422 time: 17.239962 lr: 0.400000
INFO:root:[Epoch 84] train=0.927835 val=0.858400 loss=0.203890 time: 17.224898 lr: 0.400000
INFO:root:[Epoch 85] train=0.928983 val=0.846300 loss=0.203849 time: 17.017525 lr: 0.400000
INFO:root:[Epoch 86] train=0.928681 val=0.852500 loss=0.203306 time: 17.084484 lr: 0.400000
INFO:root:[Epoch 87] train=0.926587 val=0.861900 loss=0.205383 time: 16.825093 lr: 0.400000
INFO:root:[Epoch 88] train=0.928117 val=0.847800 loss=0.200929 time: 17.279458 lr: 0.400000
INFO:root:[Epoch 89] train=0.926305 val=0.871700 loss=0.206160 time: 17.039316 lr: 0.400000
INFO:root:[Epoch 90] train=0.927855 val=0.837800 loss=0.202464 time: 17.511210 lr: 0.400000
INFO:root:[Epoch 91] train=0.927493 val=0.867400 loss=0.204233 time: 17.016278 lr: 0.400000
INFO:root:[Epoch 92] train=0.931318 val=0.831700 loss=0.194684 time: 16.610986 lr: 0.400000
INFO:root:[Epoch 93] train=0.928681 val=0.870000 loss=0.201002 time: 17.423844 lr: 0.400000
INFO:root:[Epoch 94] train=0.930896 val=0.843600 loss=0.195247 time: 17.201870 lr: 0.400000
INFO:root:[Epoch 95] train=0.930332 val=0.875100 loss=0.197273 time: 17.312303 lr: 0.400000
INFO:root:[Epoch 96] train=0.929808 val=0.842700 loss=0.198621 time: 17.392270 lr: 0.400000
INFO:root:[Epoch 97] train=0.929184 val=0.837900 loss=0.198656 time: 17.373448 lr: 0.400000
INFO:root:[Epoch 98] train=0.930614 val=0.833000 loss=0.196819 time: 17.198845 lr: 0.400000
INFO:root:[Epoch 99] train=0.927916 val=0.860900 loss=0.204382 time: 17.082774 lr: 0.400000
INFO:root:[Epoch 100] train=0.955118 val=0.909700 loss=0.132943 time: 17.402582 lr: 0.040000
INFO:root:[Epoch 101] train=0.966676 val=0.911500 loss=0.097989 time: 17.091239 lr: 0.040000
INFO:root:[Epoch 102] train=0.971025 val=0.912600 loss=0.087458 time: 17.192937 lr: 0.040000
INFO:root:[Epoch 103] train=0.972697 val=0.911800 loss=0.080864 time: 17.239326 lr: 0.040000
INFO:root:[Epoch 104] train=0.974146 val=0.913800 loss=0.077553 time: 16.947765 lr: 0.040000
INFO:root:[Epoch 105] train=0.976079 val=0.916200 loss=0.071927 time: 17.369443 lr: 0.040000
INFO:root:[Epoch 106] train=0.976965 val=0.914900 loss=0.068007 time: 16.882679 lr: 0.040000
INFO:root:[Epoch 107] train=0.977952 val=0.915900 loss=0.064582 time: 16.919554 lr: 0.040000
INFO:root:[Epoch 108] train=0.980167 val=0.914900 loss=0.061350 time: 16.934237 lr: 0.040000
INFO:root:[Epoch 109] train=0.979804 val=0.914600 loss=0.060988 time: 17.090752 lr: 0.040000
INFO:root:[Epoch 110] train=0.981898 val=0.917700 loss=0.055834 time: 17.061744 lr: 0.040000
INFO:root:[Epoch 111] train=0.982543 val=0.917600 loss=0.053992 time: 17.241749 lr: 0.040000
INFO:root:[Epoch 112] train=0.981999 val=0.915400 loss=0.053021 time: 17.253240 lr: 0.040000
INFO:root:[Epoch 113] train=0.983348 val=0.915200 loss=0.049637 time: 17.178821 lr: 0.040000
INFO:root:[Epoch 114] train=0.984053 val=0.915600 loss=0.048900 time: 17.539380 lr: 0.040000
INFO:root:[Epoch 115] train=0.984395 val=0.916300 loss=0.048051 time: 16.804920 lr: 0.040000
INFO:root:[Epoch 116] train=0.984878 val=0.917200 loss=0.045956 time: 16.962361 lr: 0.040000
INFO:root:[Epoch 117] train=0.984999 val=0.916600 loss=0.045738 time: 16.989039 lr: 0.040000
INFO:root:[Epoch 118] train=0.986509 val=0.917500 loss=0.042317 time: 17.173082 lr: 0.040000
INFO:root:[Epoch 119] train=0.985986 val=0.914000 loss=0.043205 time: 17.389799 lr: 0.040000
INFO:root:[Epoch 120] train=0.985422 val=0.916300 loss=0.043244 time: 17.096488 lr: 0.040000
INFO:root:[Epoch 121] train=0.987274 val=0.913600 loss=0.040500 time: 17.175651 lr: 0.040000
INFO:root:[Epoch 122] train=0.986852 val=0.914400 loss=0.039771 time: 17.157899 lr: 0.040000
INFO:root:[Epoch 123] train=0.986650 val=0.916600 loss=0.039647 time: 16.949711 lr: 0.040000
INFO:root:[Epoch 124] train=0.988100 val=0.917200 loss=0.037714 time: 16.925913 lr: 0.040000
INFO:root:[Epoch 125] train=0.987496 val=0.914000 loss=0.037947 time: 17.419408 lr: 0.040000
INFO:root:[Epoch 126] train=0.988462 val=0.914400 loss=0.035625 time: 17.040511 lr: 0.040000
INFO:root:[Epoch 127] train=0.989207 val=0.914400 loss=0.035052 time: 17.355468 lr: 0.040000
INFO:root:[Epoch 128] train=0.988986 val=0.915300 loss=0.035459 time: 17.550308 lr: 0.040000
INFO:root:[Epoch 129] train=0.988885 val=0.914800 loss=0.035528 time: 17.206336 lr: 0.040000
INFO:root:[Epoch 130] train=0.989107 val=0.917600 loss=0.033635 time: 16.967499 lr: 0.040000
INFO:root:[Epoch 131] train=0.989510 val=0.912900 loss=0.033109 time: 17.323532 lr: 0.040000
INFO:root:[Epoch 132] train=0.989389 val=0.913200 loss=0.032685 time: 17.372452 lr: 0.040000
INFO:root:[Epoch 133] train=0.988825 val=0.916900 loss=0.033389 time: 16.946455 lr: 0.040000
INFO:root:[Epoch 134] train=0.990758 val=0.913200 loss=0.030232 time: 17.187012 lr: 0.040000
INFO:root:[Epoch 135] train=0.990073 val=0.914500 loss=0.031122 time: 16.853546 lr: 0.040000
INFO:root:[Epoch 136] train=0.989791 val=0.912000 loss=0.032331 time: 17.522249 lr: 0.040000
INFO:root:[Epoch 137] train=0.990134 val=0.914900 loss=0.030111 time: 17.254024 lr: 0.040000
INFO:root:[Epoch 138] train=0.991181 val=0.913500 loss=0.028538 time: 16.962346 lr: 0.040000
INFO:root:[Epoch 139] train=0.990959 val=0.917000 loss=0.029347 time: 17.426689 lr: 0.040000
INFO:root:[Epoch 140] train=0.990879 val=0.913300 loss=0.028620 time: 16.712909 lr: 0.040000
INFO:root:[Epoch 141] train=0.990033 val=0.911000 loss=0.029673 time: 17.023778 lr: 0.040000
INFO:root:[Epoch 142] train=0.991261 val=0.912500 loss=0.027265 time: 17.001782 lr: 0.040000
INFO:root:[Epoch 143] train=0.991523 val=0.911800 loss=0.027788 time: 17.602697 lr: 0.040000
INFO:root:[Epoch 144] train=0.991201 val=0.915000 loss=0.026877 time: 17.498054 lr: 0.040000
INFO:root:[Epoch 145] train=0.991140 val=0.915300 loss=0.026936 time: 17.510087 lr: 0.040000
INFO:root:[Epoch 146] train=0.991302 val=0.912900 loss=0.026691 time: 17.299981 lr: 0.040000
INFO:root:[Epoch 147] train=0.991241 val=0.912100 loss=0.027214 time: 17.163764 lr: 0.040000
INFO:root:[Epoch 148] train=0.991422 val=0.913900 loss=0.027428 time: 17.085037 lr: 0.040000
INFO:root:[Epoch 149] train=0.991724 val=0.913700 loss=0.026697 time: 16.899967 lr: 0.040000
INFO:root:[Epoch 150] train=0.993617 val=0.917800 loss=0.022059 time: 17.480165 lr: 0.004000
INFO:root:[Epoch 151] train=0.994241 val=0.918300 loss=0.019709 time: 17.204319 lr: 0.004000
INFO:root:[Epoch 152] train=0.995027 val=0.917800 loss=0.018650 time: 16.858076 lr: 0.004000
INFO:root:[Epoch 153] train=0.995047 val=0.917900 loss=0.018458 time: 16.969505 lr: 0.004000
INFO:root:[Epoch 154] train=0.995127 val=0.918200 loss=0.017762 time: 17.009394 lr: 0.004000
INFO:root:[Epoch 155] train=0.994765 val=0.918000 loss=0.018511 time: 16.808652 lr: 0.004000
INFO:root:[Epoch 156] train=0.995429 val=0.917700 loss=0.017305 time: 17.382629 lr: 0.004000
INFO:root:[Epoch 157] train=0.994986 val=0.918100 loss=0.018118 time: 17.169974 lr: 0.004000
INFO:root:[Epoch 158] train=0.995751 val=0.918500 loss=0.016743 time: 16.937262 lr: 0.004000
INFO:root:[Epoch 159] train=0.995349 val=0.918400 loss=0.017128 time: 17.072854 lr: 0.004000
INFO:root:[Epoch 160] train=0.994825 val=0.918600 loss=0.017618 time: 17.423404 lr: 0.004000
INFO:root:[Epoch 161] train=0.995651 val=0.916000 loss=0.016581 time: 17.350271 lr: 0.004000
INFO:root:[Epoch 162] train=0.995288 val=0.918600 loss=0.016772 time: 17.835246 lr: 0.004000
INFO:root:[Epoch 163] train=0.996033 val=0.918400 loss=0.016359 time: 17.962824 lr: 0.004000
INFO:root:[Epoch 164] train=0.996356 val=0.918800 loss=0.015555 time: 17.207987 lr: 0.004000
INFO:root:[Epoch 165] train=0.995792 val=0.918200 loss=0.015893 time: 17.163311 lr: 0.004000
INFO:root:[Epoch 166] train=0.995651 val=0.918300 loss=0.016773 time: 17.165659 lr: 0.004000
INFO:root:[Epoch 167] train=0.996215 val=0.919000 loss=0.015229 time: 17.369281 lr: 0.004000
INFO:root:[Epoch 168] train=0.996255 val=0.918200 loss=0.015168 time: 16.972929 lr: 0.004000
INFO:root:[Epoch 169] train=0.996194 val=0.917500 loss=0.014841 time: 17.497061 lr: 0.004000
INFO:root:[Epoch 170] train=0.995933 val=0.916800 loss=0.015325 time: 17.279688 lr: 0.004000
INFO:root:[Epoch 171] train=0.996114 val=0.917900 loss=0.015141 time: 17.533211 lr: 0.004000
INFO:root:[Epoch 172] train=0.996356 val=0.918900 loss=0.015107 time: 17.076257 lr: 0.004000
INFO:root:[Epoch 173] train=0.995570 val=0.917600 loss=0.015680 time: 17.194062 lr: 0.004000
INFO:root:[Epoch 174] train=0.996174 val=0.920000 loss=0.015094 time: 17.455125 lr: 0.004000
INFO:root:[Epoch 175] train=0.996396 val=0.917900 loss=0.015052 time: 17.249531 lr: 0.004000
INFO:root:[Epoch 176] train=0.996275 val=0.917500 loss=0.015253 time: 17.031206 lr: 0.004000
INFO:root:[Epoch 177] train=0.996335 val=0.917400 loss=0.015258 time: 17.215215 lr: 0.004000
INFO:root:[Epoch 178] train=0.996295 val=0.917500 loss=0.015035 time: 17.637985 lr: 0.004000
INFO:root:[Epoch 179] train=0.996758 val=0.917400 loss=0.014112 time: 17.248820 lr: 0.004000
INFO:root:[Epoch 180] train=0.996315 val=0.917900 loss=0.014693 time: 17.605930 lr: 0.004000
INFO:root:[Epoch 181] train=0.996033 val=0.918100 loss=0.014663 time: 17.486969 lr: 0.004000
INFO:root:[Epoch 182] train=0.996094 val=0.919000 loss=0.014979 time: 17.099360 lr: 0.004000
INFO:root:[Epoch 183] train=0.996235 val=0.918500 loss=0.014389 time: 17.428729 lr: 0.004000
INFO:root:[Epoch 184] train=0.996738 val=0.918100 loss=0.014354 time: 17.690181 lr: 0.004000
INFO:root:[Epoch 185] train=0.995973 val=0.918900 loss=0.014861 time: 17.412509 lr: 0.004000
INFO:root:[Epoch 186] train=0.996376 val=0.918000 loss=0.014516 time: 17.434634 lr: 0.004000
INFO:root:[Epoch 187] train=0.996154 val=0.918700 loss=0.014309 time: 17.329184 lr: 0.004000
INFO:root:[Epoch 188] train=0.996456 val=0.918700 loss=0.014176 time: 17.026340 lr: 0.004000
INFO:root:[Epoch 189] train=0.996255 val=0.919700 loss=0.014444 time: 17.390703 lr: 0.004000
INFO:root:[Epoch 190] train=0.996617 val=0.917600 loss=0.014437 time: 17.141531 lr: 0.004000
INFO:root:[Epoch 191] train=0.996758 val=0.918500 loss=0.014336 time: 17.112930 lr: 0.004000
INFO:root:[Epoch 192] train=0.996295 val=0.918500 loss=0.014327 time: 17.112810 lr: 0.004000
INFO:root:[Epoch 193] train=0.996678 val=0.918000 loss=0.014339 time: 17.673256 lr: 0.004000
INFO:root:[Epoch 194] train=0.997020 val=0.917700 loss=0.013136 time: 17.414418 lr: 0.004000
INFO:root:[Epoch 195] train=0.996335 val=0.917500 loss=0.013980 time: 17.392243 lr: 0.004000
INFO:root:[Epoch 196] train=0.996778 val=0.916900 loss=0.013519 time: 17.195496 lr: 0.004000
INFO:root:[Epoch 197] train=0.996980 val=0.916000 loss=0.013065 time: 17.389659 lr: 0.004000
INFO:root:[Epoch 198] train=0.996658 val=0.917700 loss=0.013553 time: 17.253558 lr: 0.004000
INFO:root:[Epoch 199] train=0.996678 val=0.918400 loss=0.013791 time: 17.556382 lr: 0.004000
