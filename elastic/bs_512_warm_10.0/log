INFO:root:Namespace(batch_size=128, drop_rate=0.0, gpus='0,1,2,3', lr=0.4, lr_decay=0.1, lr_decay_epoch='100,150', mode='hybrid', model='cifar_resnet20_v1', momentum=0.9, num_epochs=200, num_workers=2, resume_from=None, save_dir='bs_512_warm_10.0', save_period=10, save_plot_dir='bs_512_warm_10.0', warmup_epochs=10, wd=0.0001)
[02:42:18] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:[Epoch 0] train=0.325608 val=0.448900 loss=1.849907 time: 18.056648 lr: 0.039588
INFO:root:[Epoch 1] train=0.546996 val=0.489100 loss=1.251044 time: 15.918239 lr: 0.079588
INFO:root:[Epoch 2] train=0.652062 val=0.581300 loss=0.975106 time: 16.105173 lr: 0.119588
INFO:root:[Epoch 3] train=0.705279 val=0.635800 loss=0.828691 time: 15.935833 lr: 0.159588
INFO:root:[Epoch 4] train=0.750805 val=0.680800 loss=0.714655 time: 15.888247 lr: 0.199588
INFO:root:[Epoch 5] train=0.778008 val=0.689700 loss=0.637846 time: 15.894661 lr: 0.239588
INFO:root:[Epoch 6] train=0.795204 val=0.651600 loss=0.591084 time: 15.646849 lr: 0.279588
INFO:root:[Epoch 7] train=0.809580 val=0.757500 loss=0.546877 time: 15.990253 lr: 0.319588
INFO:root:[Epoch 8] train=0.820635 val=0.750300 loss=0.516942 time: 16.012534 lr: 0.359588
INFO:root:[Epoch 9] train=0.829675 val=0.777700 loss=0.492788 time: 15.453009 lr: 0.399588
INFO:root:[Epoch 10] train=0.837810 val=0.773200 loss=0.469323 time: 16.001151 lr: 0.400000
INFO:root:[Epoch 11] train=0.847012 val=0.799000 loss=0.442463 time: 15.991677 lr: 0.400000
INFO:root:[Epoch 12] train=0.855892 val=0.808800 loss=0.417323 time: 15.760753 lr: 0.400000
INFO:root:[Epoch 13] train=0.859194 val=0.785000 loss=0.406148 time: 15.912417 lr: 0.400000
INFO:root:[Epoch 14] train=0.867006 val=0.794900 loss=0.386519 time: 16.160958 lr: 0.400000
INFO:root:[Epoch 15] train=0.873550 val=0.740900 loss=0.370467 time: 16.073394 lr: 0.400000
INFO:root:[Epoch 16] train=0.875060 val=0.801400 loss=0.362021 time: 16.016502 lr: 0.400000
INFO:root:[Epoch 17] train=0.879168 val=0.841700 loss=0.349373 time: 16.500607 lr: 0.400000
INFO:root:[Epoch 18] train=0.880316 val=0.792900 loss=0.344468 time: 15.741025 lr: 0.400000
INFO:root:[Epoch 19] train=0.883497 val=0.835100 loss=0.334757 time: 15.626693 lr: 0.400000
INFO:root:[Epoch 20] train=0.886034 val=0.842400 loss=0.327175 time: 15.365951 lr: 0.400000
INFO:root:[Epoch 21] train=0.888712 val=0.825900 loss=0.319158 time: 15.433029 lr: 0.400000
INFO:root:[Epoch 22] train=0.889135 val=0.848600 loss=0.316988 time: 15.679265 lr: 0.400000
INFO:root:[Epoch 23] train=0.893867 val=0.837900 loss=0.305135 time: 16.012634 lr: 0.400000
INFO:root:[Epoch 24] train=0.895780 val=0.761000 loss=0.300485 time: 16.577629 lr: 0.400000
INFO:root:[Epoch 25] train=0.895256 val=0.792300 loss=0.302106 time: 17.184326 lr: 0.400000
INFO:root:[Epoch 26] train=0.896283 val=0.777700 loss=0.292309 time: 17.357595 lr: 0.400000
INFO:root:[Epoch 27] train=0.900491 val=0.833300 loss=0.285434 time: 17.068725 lr: 0.400000
INFO:root:[Epoch 28] train=0.900129 val=0.841900 loss=0.286541 time: 17.081397 lr: 0.400000
INFO:root:[Epoch 29] train=0.902666 val=0.829500 loss=0.282314 time: 17.166593 lr: 0.400000
INFO:root:[Epoch 30] train=0.903210 val=0.786700 loss=0.278808 time: 17.500500 lr: 0.400000
INFO:root:[Epoch 31] train=0.904961 val=0.863200 loss=0.273820 time: 17.456419 lr: 0.400000
INFO:root:[Epoch 32] train=0.905767 val=0.808600 loss=0.269134 time: 16.919985 lr: 0.400000
INFO:root:[Epoch 33] train=0.904075 val=0.854100 loss=0.270996 time: 16.765744 lr: 0.400000
INFO:root:[Epoch 34] train=0.906290 val=0.851100 loss=0.267404 time: 17.277281 lr: 0.400000
INFO:root:[Epoch 35] train=0.907559 val=0.807000 loss=0.264641 time: 16.973866 lr: 0.400000
INFO:root:[Epoch 36] train=0.908465 val=0.823200 loss=0.260600 time: 17.015743 lr: 0.400000
INFO:root:[Epoch 37] train=0.907841 val=0.843300 loss=0.261675 time: 17.162049 lr: 0.400000
INFO:root:[Epoch 38] train=0.910559 val=0.853000 loss=0.255369 time: 16.972354 lr: 0.400000
INFO:root:[Epoch 39] train=0.911626 val=0.798100 loss=0.254052 time: 17.144890 lr: 0.400000
INFO:root:[Epoch 40] train=0.912190 val=0.840600 loss=0.250849 time: 16.532910 lr: 0.400000
INFO:root:[Epoch 41] train=0.912432 val=0.873400 loss=0.247597 time: 17.166045 lr: 0.400000
INFO:root:[Epoch 42] train=0.915110 val=0.767100 loss=0.245014 time: 16.974299 lr: 0.400000
INFO:root:[Epoch 43] train=0.913559 val=0.855200 loss=0.245556 time: 16.997685 lr: 0.400000
INFO:root:[Epoch 44] train=0.912673 val=0.825000 loss=0.244616 time: 17.105980 lr: 0.400000
INFO:root:[Epoch 45] train=0.916418 val=0.835000 loss=0.240011 time: 17.386809 lr: 0.400000
INFO:root:[Epoch 46] train=0.915593 val=0.827600 loss=0.242733 time: 17.303977 lr: 0.400000
INFO:root:[Epoch 47] train=0.918895 val=0.853400 loss=0.235460 time: 17.113118 lr: 0.400000
INFO:root:[Epoch 48] train=0.916660 val=0.823300 loss=0.234921 time: 16.930457 lr: 0.400000
INFO:root:[Epoch 49] train=0.917486 val=0.848500 loss=0.233534 time: 17.029910 lr: 0.400000
INFO:root:[Epoch 50] train=0.916338 val=0.841400 loss=0.237093 time: 17.167227 lr: 0.400000
INFO:root:[Epoch 51] train=0.919660 val=0.814400 loss=0.230680 time: 17.004260 lr: 0.400000
INFO:root:[Epoch 52] train=0.917989 val=0.838400 loss=0.232747 time: 16.985153 lr: 0.400000
INFO:root:[Epoch 53] train=0.919418 val=0.846800 loss=0.229335 time: 17.234123 lr: 0.400000
INFO:root:[Epoch 54] train=0.920264 val=0.830700 loss=0.230639 time: 17.120154 lr: 0.400000
INFO:root:[Epoch 55] train=0.920647 val=0.866000 loss=0.227150 time: 17.527232 lr: 0.400000
INFO:root:[Epoch 56] train=0.921211 val=0.841500 loss=0.225248 time: 16.805976 lr: 0.400000
INFO:root:[Epoch 57] train=0.922902 val=0.869100 loss=0.222584 time: 17.053993 lr: 0.400000
INFO:root:[Epoch 58] train=0.920546 val=0.857400 loss=0.225951 time: 16.928362 lr: 0.400000
INFO:root:[Epoch 59] train=0.922016 val=0.870900 loss=0.221414 time: 16.773530 lr: 0.400000
INFO:root:[Epoch 60] train=0.922117 val=0.852500 loss=0.221297 time: 17.105304 lr: 0.400000
INFO:root:[Epoch 61] train=0.921130 val=0.817100 loss=0.222163 time: 17.071989 lr: 0.400000
INFO:root:[Epoch 62] train=0.921855 val=0.848600 loss=0.221601 time: 16.991638 lr: 0.400000
INFO:root:[Epoch 63] train=0.923345 val=0.840500 loss=0.218225 time: 17.146336 lr: 0.400000
INFO:root:[Epoch 64] train=0.924029 val=0.871400 loss=0.214010 time: 16.787585 lr: 0.400000
INFO:root:[Epoch 65] train=0.923184 val=0.851200 loss=0.217209 time: 17.307648 lr: 0.400000
INFO:root:[Epoch 66] train=0.926124 val=0.853300 loss=0.211499 time: 17.259578 lr: 0.400000
INFO:root:[Epoch 67] train=0.924029 val=0.854900 loss=0.216807 time: 16.977561 lr: 0.400000
INFO:root:[Epoch 68] train=0.924170 val=0.855100 loss=0.217322 time: 17.018265 lr: 0.400000
INFO:root:[Epoch 69] train=0.926164 val=0.855100 loss=0.212254 time: 17.039440 lr: 0.400000
INFO:root:[Epoch 70] train=0.924009 val=0.871000 loss=0.216051 time: 16.834348 lr: 0.400000
INFO:root:[Epoch 71] train=0.926083 val=0.819400 loss=0.208461 time: 17.077567 lr: 0.400000
INFO:root:[Epoch 72] train=0.927050 val=0.852200 loss=0.208801 time: 17.564996 lr: 0.400000
INFO:root:[Epoch 73] train=0.923868 val=0.842200 loss=0.211939 time: 17.168521 lr: 0.400000
INFO:root:[Epoch 74] train=0.925862 val=0.858900 loss=0.212321 time: 17.087357 lr: 0.400000
INFO:root:[Epoch 75] train=0.926305 val=0.835800 loss=0.209968 time: 17.008039 lr: 0.400000
INFO:root:[Epoch 76] train=0.925439 val=0.846300 loss=0.211857 time: 17.458958 lr: 0.400000
INFO:root:[Epoch 77] train=0.926768 val=0.846600 loss=0.209583 time: 17.643910 lr: 0.400000
INFO:root:[Epoch 78] train=0.926607 val=0.858200 loss=0.210033 time: 17.224951 lr: 0.400000
INFO:root:[Epoch 79] train=0.926103 val=0.828100 loss=0.210355 time: 17.051272 lr: 0.400000
INFO:root:[Epoch 80] train=0.927473 val=0.801700 loss=0.205291 time: 17.142808 lr: 0.400000
INFO:root:[Epoch 81] train=0.925902 val=0.846500 loss=0.210005 time: 17.398672 lr: 0.400000
INFO:root:[Epoch 82] train=0.928922 val=0.866000 loss=0.203607 time: 17.090855 lr: 0.400000
INFO:root:[Epoch 83] train=0.928016 val=0.866000 loss=0.206272 time: 17.066653 lr: 0.400000
INFO:root:[Epoch 84] train=0.928318 val=0.863600 loss=0.203865 time: 17.281405 lr: 0.400000
INFO:root:[Epoch 85] train=0.927452 val=0.849000 loss=0.206892 time: 17.257337 lr: 0.400000
INFO:root:[Epoch 86] train=0.929083 val=0.871300 loss=0.200901 time: 16.910946 lr: 0.400000
INFO:root:[Epoch 87] train=0.928540 val=0.842600 loss=0.202266 time: 17.159711 lr: 0.400000
INFO:root:[Epoch 88] train=0.927815 val=0.865600 loss=0.199599 time: 17.642435 lr: 0.400000
INFO:root:[Epoch 89] train=0.928681 val=0.835400 loss=0.203368 time: 17.221192 lr: 0.400000
INFO:root:[Epoch 90] train=0.929728 val=0.848300 loss=0.199910 time: 17.106775 lr: 0.400000
INFO:root:[Epoch 91] train=0.929365 val=0.882800 loss=0.201337 time: 17.593732 lr: 0.400000
INFO:root:[Epoch 92] train=0.930594 val=0.828400 loss=0.196727 time: 17.401306 lr: 0.400000
INFO:root:[Epoch 93] train=0.928902 val=0.813700 loss=0.201600 time: 17.277304 lr: 0.400000
INFO:root:[Epoch 94] train=0.930372 val=0.865600 loss=0.198750 time: 17.198814 lr: 0.400000
INFO:root:[Epoch 95] train=0.932144 val=0.828000 loss=0.194624 time: 17.013478 lr: 0.400000
INFO:root:[Epoch 96] train=0.929909 val=0.857400 loss=0.199428 time: 17.042500 lr: 0.400000
INFO:root:[Epoch 97] train=0.929969 val=0.863000 loss=0.199857 time: 16.833223 lr: 0.400000
INFO:root:[Epoch 98] train=0.930835 val=0.851300 loss=0.194292 time: 17.228820 lr: 0.400000
INFO:root:[Epoch 99] train=0.930654 val=0.869200 loss=0.200019 time: 16.978702 lr: 0.400000
INFO:root:[Epoch 100] train=0.955944 val=0.913700 loss=0.129435 time: 17.059960 lr: 0.040000
INFO:root:[Epoch 101] train=0.965710 val=0.915700 loss=0.100522 time: 17.366362 lr: 0.040000
INFO:root:[Epoch 102] train=0.970321 val=0.915000 loss=0.088699 time: 17.376750 lr: 0.040000
INFO:root:[Epoch 103] train=0.973723 val=0.915600 loss=0.079926 time: 17.203598 lr: 0.040000
INFO:root:[Epoch 104] train=0.975354 val=0.914100 loss=0.075327 time: 16.822567 lr: 0.040000
INFO:root:[Epoch 105] train=0.977388 val=0.917300 loss=0.069018 time: 17.321354 lr: 0.040000
INFO:root:[Epoch 106] train=0.977811 val=0.915000 loss=0.066936 time: 17.502569 lr: 0.040000
INFO:root:[Epoch 107] train=0.978576 val=0.917200 loss=0.063515 time: 17.290675 lr: 0.040000
INFO:root:[Epoch 108] train=0.979240 val=0.916700 loss=0.062013 time: 16.951444 lr: 0.040000
INFO:root:[Epoch 109] train=0.981516 val=0.915200 loss=0.058127 time: 16.794386 lr: 0.040000
INFO:root:[Epoch 110] train=0.981838 val=0.918000 loss=0.056137 time: 17.437444 lr: 0.040000
INFO:root:[Epoch 111] train=0.982039 val=0.917500 loss=0.053935 time: 16.968326 lr: 0.040000
INFO:root:[Epoch 112] train=0.981999 val=0.917600 loss=0.051228 time: 17.011780 lr: 0.040000
INFO:root:[Epoch 113] train=0.983610 val=0.915700 loss=0.049487 time: 17.377234 lr: 0.040000
INFO:root:[Epoch 114] train=0.984133 val=0.917900 loss=0.049092 time: 17.243817 lr: 0.040000
INFO:root:[Epoch 115] train=0.984254 val=0.918100 loss=0.046924 time: 17.374179 lr: 0.040000
INFO:root:[Epoch 116] train=0.984294 val=0.917800 loss=0.047633 time: 17.091893 lr: 0.040000
INFO:root:[Epoch 117] train=0.985583 val=0.917100 loss=0.043560 time: 17.418207 lr: 0.040000
INFO:root:[Epoch 118] train=0.986147 val=0.919500 loss=0.043038 time: 16.937689 lr: 0.040000
INFO:root:[Epoch 119] train=0.985704 val=0.915900 loss=0.043899 time: 16.776286 lr: 0.040000
INFO:root:[Epoch 120] train=0.986449 val=0.914600 loss=0.041235 time: 17.214023 lr: 0.040000
INFO:root:[Epoch 121] train=0.987073 val=0.914700 loss=0.040029 time: 16.950025 lr: 0.040000
INFO:root:[Epoch 122] train=0.986993 val=0.913500 loss=0.038956 time: 17.298591 lr: 0.040000
INFO:root:[Epoch 123] train=0.987415 val=0.915800 loss=0.038928 time: 17.062869 lr: 0.040000
INFO:root:[Epoch 124] train=0.988160 val=0.916300 loss=0.037110 time: 17.278738 lr: 0.040000
INFO:root:[Epoch 125] train=0.987798 val=0.915800 loss=0.037111 time: 17.295134 lr: 0.040000
INFO:root:[Epoch 126] train=0.988603 val=0.915400 loss=0.036483 time: 17.262325 lr: 0.040000
INFO:root:[Epoch 127] train=0.987818 val=0.916100 loss=0.036243 time: 17.224726 lr: 0.040000
INFO:root:[Epoch 128] train=0.989389 val=0.914700 loss=0.034177 time: 17.469246 lr: 0.040000
INFO:root:[Epoch 129] train=0.989510 val=0.914100 loss=0.033546 time: 18.118809 lr: 0.040000
INFO:root:[Epoch 130] train=0.989389 val=0.916900 loss=0.032167 time: 16.998790 lr: 0.040000
INFO:root:[Epoch 131] train=0.989409 val=0.914400 loss=0.033702 time: 16.808264 lr: 0.040000
INFO:root:[Epoch 132] train=0.990315 val=0.916400 loss=0.031798 time: 17.231838 lr: 0.040000
INFO:root:[Epoch 133] train=0.990013 val=0.915200 loss=0.031319 time: 17.268402 lr: 0.040000
INFO:root:[Epoch 134] train=0.989449 val=0.916400 loss=0.032595 time: 16.908560 lr: 0.040000
INFO:root:[Epoch 135] train=0.990758 val=0.914800 loss=0.029836 time: 17.219642 lr: 0.040000
INFO:root:[Epoch 136] train=0.990718 val=0.916100 loss=0.029612 time: 17.004656 lr: 0.040000
INFO:root:[Epoch 137] train=0.990758 val=0.916700 loss=0.029575 time: 17.239654 lr: 0.040000
INFO:root:[Epoch 138] train=0.989550 val=0.917300 loss=0.030583 time: 17.162225 lr: 0.040000
INFO:root:[Epoch 139] train=0.991161 val=0.915200 loss=0.028101 time: 17.351212 lr: 0.040000
INFO:root:[Epoch 140] train=0.990436 val=0.914100 loss=0.029401 time: 17.075530 lr: 0.040000
INFO:root:[Epoch 141] train=0.990778 val=0.914600 loss=0.029460 time: 17.211869 lr: 0.040000
INFO:root:[Epoch 142] train=0.990859 val=0.915000 loss=0.027762 time: 16.984529 lr: 0.040000
INFO:root:[Epoch 143] train=0.991322 val=0.917500 loss=0.027598 time: 17.271329 lr: 0.040000
INFO:root:[Epoch 144] train=0.991221 val=0.914600 loss=0.027676 time: 17.006759 lr: 0.040000
INFO:root:[Epoch 145] train=0.991241 val=0.914000 loss=0.026711 time: 17.229328 lr: 0.040000
INFO:root:[Epoch 146] train=0.991382 val=0.911800 loss=0.027529 time: 17.426307 lr: 0.040000
INFO:root:[Epoch 147] train=0.991664 val=0.912200 loss=0.026622 time: 17.538041 lr: 0.040000
INFO:root:[Epoch 148] train=0.992933 val=0.913100 loss=0.023999 time: 17.325548 lr: 0.040000
INFO:root:[Epoch 149] train=0.991362 val=0.910800 loss=0.027111 time: 17.140056 lr: 0.040000
INFO:root:[Epoch 150] train=0.993235 val=0.917100 loss=0.022262 time: 17.353464 lr: 0.004000
INFO:root:[Epoch 151] train=0.994141 val=0.916300 loss=0.020613 time: 17.297076 lr: 0.004000
INFO:root:[Epoch 152] train=0.994221 val=0.917700 loss=0.019469 time: 17.108064 lr: 0.004000
INFO:root:[Epoch 153] train=0.994664 val=0.917600 loss=0.018450 time: 17.106987 lr: 0.004000
INFO:root:[Epoch 154] train=0.995107 val=0.917800 loss=0.017948 time: 16.950881 lr: 0.004000
INFO:root:[Epoch 155] train=0.995027 val=0.918000 loss=0.018267 time: 17.240135 lr: 0.004000
INFO:root:[Epoch 156] train=0.995731 val=0.917900 loss=0.016952 time: 17.403578 lr: 0.004000
INFO:root:[Epoch 157] train=0.995268 val=0.916900 loss=0.017107 time: 17.076504 lr: 0.004000
INFO:root:[Epoch 158] train=0.995268 val=0.917200 loss=0.017820 time: 16.993250 lr: 0.004000
INFO:root:[Epoch 159] train=0.995308 val=0.917000 loss=0.017005 time: 17.219095 lr: 0.004000
INFO:root:[Epoch 160] train=0.995631 val=0.917600 loss=0.016324 time: 17.033775 lr: 0.004000
INFO:root:[Epoch 161] train=0.995590 val=0.916600 loss=0.016692 time: 17.555182 lr: 0.004000
INFO:root:[Epoch 162] train=0.995872 val=0.917600 loss=0.016014 time: 17.341251 lr: 0.004000
INFO:root:[Epoch 163] train=0.995812 val=0.918300 loss=0.016279 time: 17.492543 lr: 0.004000
INFO:root:[Epoch 164] train=0.995852 val=0.917500 loss=0.016117 time: 17.573637 lr: 0.004000
INFO:root:[Epoch 165] train=0.995550 val=0.917800 loss=0.016700 time: 17.523403 lr: 0.004000
INFO:root:[Epoch 166] train=0.995409 val=0.918400 loss=0.016501 time: 17.645486 lr: 0.004000
INFO:root:[Epoch 167] train=0.995772 val=0.918100 loss=0.016658 time: 16.992697 lr: 0.004000
INFO:root:[Epoch 168] train=0.996134 val=0.918100 loss=0.015522 time: 17.103914 lr: 0.004000
INFO:root:[Epoch 169] train=0.995812 val=0.917900 loss=0.016440 time: 17.313476 lr: 0.004000
INFO:root:[Epoch 170] train=0.995429 val=0.918500 loss=0.016224 time: 16.903456 lr: 0.004000
INFO:root:[Epoch 171] train=0.995832 val=0.918800 loss=0.016232 time: 17.299132 lr: 0.004000
INFO:root:[Epoch 172] train=0.995590 val=0.918000 loss=0.016089 time: 17.461243 lr: 0.004000
INFO:root:[Epoch 173] train=0.995731 val=0.917500 loss=0.015827 time: 17.211922 lr: 0.004000
INFO:root:[Epoch 174] train=0.996053 val=0.920500 loss=0.015513 time: 17.605302 lr: 0.004000
INFO:root:[Epoch 175] train=0.996376 val=0.918900 loss=0.014978 time: 17.017984 lr: 0.004000
INFO:root:[Epoch 176] train=0.996496 val=0.917900 loss=0.014842 time: 17.397756 lr: 0.004000
INFO:root:[Epoch 177] train=0.996275 val=0.916900 loss=0.015103 time: 17.019276 lr: 0.004000
INFO:root:[Epoch 178] train=0.996295 val=0.919000 loss=0.014873 time: 17.489580 lr: 0.004000
INFO:root:[Epoch 179] train=0.996033 val=0.918100 loss=0.015736 time: 17.085415 lr: 0.004000
INFO:root:[Epoch 180] train=0.995953 val=0.917600 loss=0.015392 time: 17.178744 lr: 0.004000
INFO:root:[Epoch 181] train=0.996295 val=0.917100 loss=0.014788 time: 17.422801 lr: 0.004000
INFO:root:[Epoch 182] train=0.996215 val=0.918200 loss=0.015000 time: 17.304829 lr: 0.004000
INFO:root:[Epoch 183] train=0.996235 val=0.917900 loss=0.015130 time: 18.176714 lr: 0.004000
INFO:root:[Epoch 184] train=0.995711 val=0.915900 loss=0.015719 time: 16.984922 lr: 0.004000
INFO:root:[Epoch 185] train=0.995852 val=0.919000 loss=0.015175 time: 17.127185 lr: 0.004000
INFO:root:[Epoch 186] train=0.996315 val=0.917400 loss=0.014009 time: 17.334843 lr: 0.004000
INFO:root:[Epoch 187] train=0.996637 val=0.919800 loss=0.013804 time: 17.581085 lr: 0.004000
INFO:root:[Epoch 188] train=0.996577 val=0.918000 loss=0.014193 time: 17.432852 lr: 0.004000
INFO:root:[Epoch 189] train=0.996255 val=0.918900 loss=0.014514 time: 16.667956 lr: 0.004000
INFO:root:[Epoch 190] train=0.996879 val=0.919400 loss=0.013218 time: 16.290971 lr: 0.004000
INFO:root:[Epoch 191] train=0.996335 val=0.919300 loss=0.013894 time: 16.333830 lr: 0.004000
INFO:root:[Epoch 192] train=0.996074 val=0.917100 loss=0.014864 time: 16.714514 lr: 0.004000
INFO:root:[Epoch 193] train=0.996517 val=0.917800 loss=0.014360 time: 16.970688 lr: 0.004000
INFO:root:[Epoch 194] train=0.996033 val=0.917600 loss=0.014483 time: 17.112246 lr: 0.004000
INFO:root:[Epoch 195] train=0.996637 val=0.917200 loss=0.013423 time: 16.834598 lr: 0.004000
INFO:root:[Epoch 196] train=0.996235 val=0.917700 loss=0.014456 time: 16.274530 lr: 0.004000
INFO:root:[Epoch 197] train=0.996295 val=0.917400 loss=0.013966 time: 16.123029 lr: 0.004000
INFO:root:[Epoch 198] train=0.996376 val=0.917200 loss=0.014153 time: 16.401749 lr: 0.004000
INFO:root:[Epoch 199] train=0.996637 val=0.918400 loss=0.013862 time: 16.378113 lr: 0.004000
