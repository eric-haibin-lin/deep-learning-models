INFO:root:Namespace(batch_size=512, drop_rate=0.0, gpus='0', lr=0.4, lr_decay=0.1, lr_decay_epoch='100,150', mode='hybrid', model='cifar_resnet20_v1', momentum=0.9, num_epochs=200, num_workers=2, resume_from=None, save_dir='bs_512_gpu_1_warm_10.0', save_period=10, save_plot_dir='bs_512_gpu_1_warm_10.0', warmup_epochs=10, wd=0.0001)
[02:48:30] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:[Epoch 0] train=0.319769 val=0.417400 loss=1.856019 time: 16.312314 lr: 0.039588
INFO:root:[Epoch 1] train=0.548707 val=0.536000 loss=1.241375 time: 16.501649 lr: 0.079588
INFO:root:[Epoch 2] train=0.651619 val=0.594300 loss=0.978796 time: 16.366309 lr: 0.119588
INFO:root:[Epoch 3] train=0.709085 val=0.654400 loss=0.821398 time: 16.736112 lr: 0.159588
INFO:root:[Epoch 4] train=0.749819 val=0.739300 loss=0.715254 time: 16.908617 lr: 0.199588
INFO:root:[Epoch 5] train=0.775612 val=0.677300 loss=0.645153 time: 16.393536 lr: 0.239588
INFO:root:[Epoch 6] train=0.797358 val=0.767400 loss=0.586926 time: 16.454957 lr: 0.279588
INFO:root:[Epoch 7] train=0.811091 val=0.705700 loss=0.546572 time: 16.474308 lr: 0.319588
INFO:root:[Epoch 8] train=0.819990 val=0.749000 loss=0.518791 time: 16.459846 lr: 0.359588
INFO:root:[Epoch 9] train=0.830642 val=0.807000 loss=0.490370 time: 16.499352 lr: 0.399588
INFO:root:[Epoch 10] train=0.839663 val=0.793900 loss=0.463850 time: 16.282895 lr: 0.400000
INFO:root:[Epoch 11] train=0.845502 val=0.792400 loss=0.443124 time: 16.331929 lr: 0.400000
INFO:root:[Epoch 12] train=0.860764 val=0.776400 loss=0.409404 time: 16.292261 lr: 0.400000
INFO:root:[Epoch 13] train=0.861348 val=0.834900 loss=0.397491 time: 16.417403 lr: 0.400000
INFO:root:[Epoch 14] train=0.866563 val=0.750000 loss=0.383380 time: 16.548242 lr: 0.400000
INFO:root:[Epoch 15] train=0.872060 val=0.812900 loss=0.368879 time: 16.501177 lr: 0.400000
INFO:root:[Epoch 16] train=0.876510 val=0.829200 loss=0.355816 time: 16.224906 lr: 0.400000
INFO:root:[Epoch 17] train=0.881625 val=0.805200 loss=0.345590 time: 16.504945 lr: 0.400000
INFO:root:[Epoch 18] train=0.881685 val=0.808200 loss=0.338788 time: 16.445686 lr: 0.400000
INFO:root:[Epoch 19] train=0.885329 val=0.813700 loss=0.327804 time: 16.769321 lr: 0.400000
INFO:root:[Epoch 20] train=0.887726 val=0.815600 loss=0.324088 time: 16.588773 lr: 0.400000
INFO:root:[Epoch 21] train=0.891330 val=0.789200 loss=0.311739 time: 16.570019 lr: 0.400000
INFO:root:[Epoch 22] train=0.895014 val=0.746800 loss=0.304369 time: 16.668674 lr: 0.400000
INFO:root:[Epoch 23] train=0.893102 val=0.825900 loss=0.301599 time: 16.747628 lr: 0.400000
INFO:root:[Epoch 24] train=0.899485 val=0.831400 loss=0.292060 time: 16.700154 lr: 0.400000
INFO:root:[Epoch 25] train=0.895840 val=0.826600 loss=0.294231 time: 16.250873 lr: 0.400000
INFO:root:[Epoch 26] train=0.900310 val=0.777100 loss=0.286163 time: 16.899834 lr: 0.400000
INFO:root:[Epoch 27] train=0.900028 val=0.808200 loss=0.281954 time: 16.270453 lr: 0.400000
INFO:root:[Epoch 28] train=0.903451 val=0.843100 loss=0.274775 time: 16.642981 lr: 0.400000
INFO:root:[Epoch 29] train=0.905626 val=0.790700 loss=0.275686 time: 16.689910 lr: 0.400000
INFO:root:[Epoch 30] train=0.905163 val=0.848600 loss=0.272409 time: 16.257262 lr: 0.400000
INFO:root:[Epoch 31] train=0.907458 val=0.827100 loss=0.265134 time: 16.786401 lr: 0.400000
INFO:root:[Epoch 32] train=0.906874 val=0.845700 loss=0.262650 time: 16.581494 lr: 0.400000
INFO:root:[Epoch 33] train=0.908284 val=0.819400 loss=0.260574 time: 16.556735 lr: 0.400000
INFO:root:[Epoch 34] train=0.910720 val=0.843400 loss=0.255633 time: 16.707296 lr: 0.400000
INFO:root:[Epoch 35] train=0.909713 val=0.833300 loss=0.257645 time: 17.456880 lr: 0.400000
INFO:root:[Epoch 36] train=0.911968 val=0.844400 loss=0.250893 time: 16.594126 lr: 0.400000
INFO:root:[Epoch 37] train=0.912130 val=0.856700 loss=0.251746 time: 16.476295 lr: 0.400000
INFO:root:[Epoch 38] train=0.914324 val=0.817700 loss=0.246467 time: 16.401181 lr: 0.400000
INFO:root:[Epoch 39] train=0.913740 val=0.848300 loss=0.245870 time: 16.506303 lr: 0.400000
INFO:root:[Epoch 40] train=0.915492 val=0.832200 loss=0.238598 time: 17.074369 lr: 0.400000
INFO:root:[Epoch 41] train=0.915734 val=0.841000 loss=0.243470 time: 16.435766 lr: 0.400000
INFO:root:[Epoch 42] train=0.915613 val=0.834800 loss=0.240419 time: 16.557430 lr: 0.400000
INFO:root:[Epoch 43] train=0.917103 val=0.804500 loss=0.236778 time: 17.127227 lr: 0.400000
INFO:root:[Epoch 44] train=0.918251 val=0.837400 loss=0.232580 time: 16.969929 lr: 0.400000
INFO:root:[Epoch 45] train=0.918633 val=0.839000 loss=0.232166 time: 16.870437 lr: 0.400000
INFO:root:[Epoch 46] train=0.918875 val=0.864800 loss=0.229085 time: 16.908677 lr: 0.400000
INFO:root:[Epoch 47] train=0.919741 val=0.833800 loss=0.228059 time: 16.461524 lr: 0.400000
INFO:root:[Epoch 48] train=0.918049 val=0.799200 loss=0.231147 time: 16.608644 lr: 0.400000
INFO:root:[Epoch 49] train=0.920466 val=0.851000 loss=0.223634 time: 16.720860 lr: 0.400000
INFO:root:[Epoch 50] train=0.919982 val=0.808800 loss=0.226833 time: 16.537025 lr: 0.400000
INFO:root:[Epoch 51] train=0.918673 val=0.820300 loss=0.230666 time: 16.521097 lr: 0.400000
INFO:root:[Epoch 52] train=0.923083 val=0.855700 loss=0.219741 time: 17.089230 lr: 0.400000
INFO:root:[Epoch 53] train=0.921251 val=0.847100 loss=0.225948 time: 16.788043 lr: 0.400000
INFO:root:[Epoch 54] train=0.921190 val=0.848100 loss=0.222927 time: 16.802519 lr: 0.400000
INFO:root:[Epoch 55] train=0.923264 val=0.846600 loss=0.215741 time: 16.697658 lr: 0.400000
INFO:root:[Epoch 56] train=0.922801 val=0.837100 loss=0.218458 time: 16.440680 lr: 0.400000
INFO:root:[Epoch 57] train=0.923184 val=0.785800 loss=0.217693 time: 16.411425 lr: 0.400000
INFO:root:[Epoch 58] train=0.925379 val=0.871100 loss=0.216841 time: 16.783798 lr: 0.400000
INFO:root:[Epoch 59] train=0.925117 val=0.865800 loss=0.210730 time: 16.702258 lr: 0.400000
INFO:root:[Epoch 60] train=0.926627 val=0.821900 loss=0.210274 time: 16.553315 lr: 0.400000
INFO:root:[Epoch 61] train=0.924211 val=0.844600 loss=0.215507 time: 16.698406 lr: 0.400000
INFO:root:[Epoch 62] train=0.923969 val=0.827300 loss=0.214915 time: 16.447255 lr: 0.400000
INFO:root:[Epoch 63] train=0.926909 val=0.852800 loss=0.208420 time: 17.050612 lr: 0.400000
INFO:root:[Epoch 64] train=0.926103 val=0.843400 loss=0.208970 time: 16.765573 lr: 0.400000
INFO:root:[Epoch 65] train=0.925137 val=0.801400 loss=0.212773 time: 16.667510 lr: 0.400000
INFO:root:[Epoch 66] train=0.925701 val=0.860600 loss=0.210068 time: 16.482468 lr: 0.400000
INFO:root:[Epoch 67] train=0.930151 val=0.840500 loss=0.199950 time: 16.316762 lr: 0.400000
INFO:root:[Epoch 68] train=0.928983 val=0.863400 loss=0.202114 time: 16.758846 lr: 0.400000
INFO:root:[Epoch 69] train=0.928540 val=0.840600 loss=0.204781 time: 16.647938 lr: 0.400000
INFO:root:[Epoch 70] train=0.925278 val=0.828500 loss=0.210345 time: 16.696148 lr: 0.400000
INFO:root:[Epoch 71] train=0.927150 val=0.845200 loss=0.209106 time: 16.914694 lr: 0.400000
INFO:root:[Epoch 72] train=0.927996 val=0.851000 loss=0.205106 time: 16.790555 lr: 0.400000
INFO:root:[Epoch 73] train=0.929184 val=0.842700 loss=0.201866 time: 17.223225 lr: 0.400000
INFO:root:[Epoch 74] train=0.929224 val=0.853400 loss=0.199396 time: 16.910068 lr: 0.400000
INFO:root:[Epoch 75] train=0.928862 val=0.870500 loss=0.203313 time: 16.280653 lr: 0.400000
INFO:root:[Epoch 76] train=0.930875 val=0.849200 loss=0.198690 time: 16.453577 lr: 0.400000
INFO:root:[Epoch 77] train=0.928983 val=0.838200 loss=0.200742 time: 16.519109 lr: 0.400000
INFO:root:[Epoch 78] train=0.929426 val=0.854500 loss=0.200119 time: 17.006196 lr: 0.400000
INFO:root:[Epoch 79] train=0.928761 val=0.846100 loss=0.200551 time: 16.769685 lr: 0.400000
INFO:root:[Epoch 80] train=0.929426 val=0.852200 loss=0.200297 time: 16.326368 lr: 0.400000
INFO:root:[Epoch 81] train=0.929083 val=0.802000 loss=0.199376 time: 16.770149 lr: 0.400000
INFO:root:[Epoch 82] train=0.931137 val=0.823900 loss=0.196089 time: 16.444612 lr: 0.400000
INFO:root:[Epoch 83] train=0.928459 val=0.818500 loss=0.201165 time: 16.265683 lr: 0.400000
INFO:root:[Epoch 84] train=0.932607 val=0.830900 loss=0.192487 time: 16.247027 lr: 0.400000
INFO:root:[Epoch 85] train=0.930533 val=0.825100 loss=0.195593 time: 16.624249 lr: 0.400000
INFO:root:[Epoch 86] train=0.931218 val=0.845600 loss=0.196398 time: 16.505260 lr: 0.400000
INFO:root:[Epoch 87] train=0.930473 val=0.849500 loss=0.197366 time: 16.685520 lr: 0.400000
INFO:root:[Epoch 88] train=0.929023 val=0.843900 loss=0.199119 time: 16.803031 lr: 0.400000
INFO:root:[Epoch 89] train=0.931278 val=0.815900 loss=0.194680 time: 16.529166 lr: 0.400000
INFO:root:[Epoch 90] train=0.928661 val=0.813000 loss=0.200617 time: 16.486696 lr: 0.400000
INFO:root:[Epoch 91] train=0.932446 val=0.853600 loss=0.191756 time: 16.655917 lr: 0.400000
INFO:root:[Epoch 92] train=0.932285 val=0.860600 loss=0.193921 time: 16.838875 lr: 0.400000
INFO:root:[Epoch 93] train=0.932949 val=0.880400 loss=0.190734 time: 16.703087 lr: 0.400000
INFO:root:[Epoch 94] train=0.932043 val=0.820000 loss=0.192157 time: 16.938917 lr: 0.400000
INFO:root:[Epoch 95] train=0.931923 val=0.828700 loss=0.193112 time: 16.994132 lr: 0.400000
INFO:root:[Epoch 96] train=0.933413 val=0.841000 loss=0.188236 time: 17.022069 lr: 0.400000
INFO:root:[Epoch 97] train=0.932688 val=0.868500 loss=0.193413 time: 16.813548 lr: 0.400000
INFO:root:[Epoch 98] train=0.934601 val=0.862300 loss=0.186238 time: 16.540769 lr: 0.400000
INFO:root:[Epoch 99] train=0.932063 val=0.847700 loss=0.190442 time: 16.549638 lr: 0.400000
INFO:root:[Epoch 100] train=0.959367 val=0.913000 loss=0.117961 time: 16.641380 lr: 0.040000
INFO:root:[Epoch 101] train=0.971629 val=0.911000 loss=0.087715 time: 16.731323 lr: 0.040000
INFO:root:[Epoch 102] train=0.973844 val=0.915800 loss=0.079070 time: 17.097962 lr: 0.040000
INFO:root:[Epoch 103] train=0.975717 val=0.915400 loss=0.072351 time: 16.747854 lr: 0.040000
INFO:root:[Epoch 104] train=0.978173 val=0.914700 loss=0.066711 time: 16.613523 lr: 0.040000
INFO:root:[Epoch 105] train=0.979422 val=0.912500 loss=0.062055 time: 16.454898 lr: 0.040000
INFO:root:[Epoch 106] train=0.981113 val=0.915200 loss=0.058417 time: 16.850132 lr: 0.040000
INFO:root:[Epoch 107] train=0.981395 val=0.914200 loss=0.055822 time: 17.008828 lr: 0.040000
INFO:root:[Epoch 108] train=0.983006 val=0.915000 loss=0.053108 time: 16.621928 lr: 0.040000
INFO:root:[Epoch 109] train=0.982502 val=0.914800 loss=0.052241 time: 17.029738 lr: 0.040000
INFO:root:[Epoch 110] train=0.983046 val=0.913400 loss=0.050242 time: 16.337885 lr: 0.040000
INFO:root:[Epoch 111] train=0.984959 val=0.915800 loss=0.046059 time: 16.826741 lr: 0.040000
INFO:root:[Epoch 112] train=0.985341 val=0.915500 loss=0.045975 time: 16.924117 lr: 0.040000
INFO:root:[Epoch 113] train=0.986288 val=0.913100 loss=0.041659 time: 16.540033 lr: 0.040000
INFO:root:[Epoch 114] train=0.986187 val=0.914700 loss=0.042041 time: 16.616326 lr: 0.040000
INFO:root:[Epoch 115] train=0.988503 val=0.914100 loss=0.036846 time: 16.754866 lr: 0.040000
INFO:root:[Epoch 116] train=0.987013 val=0.915900 loss=0.038684 time: 16.964897 lr: 0.040000
INFO:root:[Epoch 117] train=0.987798 val=0.914600 loss=0.036930 time: 17.020758 lr: 0.040000
INFO:root:[Epoch 118] train=0.988060 val=0.915000 loss=0.036106 time: 16.514976 lr: 0.040000
INFO:root:[Epoch 119] train=0.988040 val=0.914800 loss=0.036944 time: 16.907776 lr: 0.040000
INFO:root:[Epoch 120] train=0.988503 val=0.914900 loss=0.035301 time: 16.477956 lr: 0.040000
INFO:root:[Epoch 121] train=0.988865 val=0.914700 loss=0.034492 time: 16.754693 lr: 0.040000
INFO:root:[Epoch 122] train=0.990335 val=0.911600 loss=0.030955 time: 16.543045 lr: 0.040000
INFO:root:[Epoch 123] train=0.990395 val=0.913900 loss=0.030652 time: 16.611655 lr: 0.040000
INFO:root:[Epoch 124] train=0.990476 val=0.913300 loss=0.031074 time: 16.517131 lr: 0.040000
INFO:root:[Epoch 125] train=0.990939 val=0.912700 loss=0.029445 time: 16.711413 lr: 0.040000
INFO:root:[Epoch 126] train=0.991120 val=0.912200 loss=0.028197 time: 17.007726 lr: 0.040000
INFO:root:[Epoch 127] train=0.989671 val=0.914200 loss=0.030354 time: 16.597700 lr: 0.040000
INFO:root:[Epoch 128] train=0.991080 val=0.914400 loss=0.028356 time: 16.489104 lr: 0.040000
INFO:root:[Epoch 129] train=0.992006 val=0.915100 loss=0.025814 time: 16.754615 lr: 0.040000
INFO:root:[Epoch 130] train=0.992228 val=0.914400 loss=0.025865 time: 16.238657 lr: 0.040000
INFO:root:[Epoch 131] train=0.992268 val=0.914300 loss=0.025339 time: 16.532109 lr: 0.040000
INFO:root:[Epoch 132] train=0.992127 val=0.916000 loss=0.025496 time: 16.656206 lr: 0.040000
INFO:root:[Epoch 133] train=0.992570 val=0.914000 loss=0.024670 time: 17.040945 lr: 0.040000
INFO:root:[Epoch 134] train=0.992268 val=0.913800 loss=0.023959 time: 16.799052 lr: 0.040000
INFO:root:[Epoch 135] train=0.992510 val=0.911100 loss=0.023879 time: 16.886520 lr: 0.040000
INFO:root:[Epoch 136] train=0.992651 val=0.911600 loss=0.024092 time: 16.786056 lr: 0.040000
INFO:root:[Epoch 137] train=0.992973 val=0.915500 loss=0.023232 time: 16.769426 lr: 0.040000
INFO:root:[Epoch 138] train=0.993516 val=0.915200 loss=0.022579 time: 16.881213 lr: 0.040000
INFO:root:[Epoch 139] train=0.993516 val=0.913900 loss=0.021833 time: 16.743156 lr: 0.040000
INFO:root:[Epoch 140] train=0.992872 val=0.913900 loss=0.023187 time: 16.183904 lr: 0.040000
INFO:root:[Epoch 141] train=0.993557 val=0.915400 loss=0.021488 time: 16.784968 lr: 0.040000
INFO:root:[Epoch 142] train=0.993416 val=0.914800 loss=0.020952 time: 16.705527 lr: 0.040000
INFO:root:[Epoch 143] train=0.992892 val=0.914900 loss=0.021416 time: 16.412906 lr: 0.040000
INFO:root:[Epoch 144] train=0.993818 val=0.911100 loss=0.020991 time: 16.663505 lr: 0.040000
INFO:root:[Epoch 145] train=0.994060 val=0.914600 loss=0.020558 time: 16.628478 lr: 0.040000
INFO:root:[Epoch 146] train=0.994000 val=0.906400 loss=0.020373 time: 16.744282 lr: 0.040000
INFO:root:[Epoch 147] train=0.993879 val=0.912000 loss=0.020129 time: 16.969128 lr: 0.040000
INFO:root:[Epoch 148] train=0.994261 val=0.914900 loss=0.018988 time: 16.618544 lr: 0.040000
INFO:root:[Epoch 149] train=0.994161 val=0.911600 loss=0.019012 time: 16.807097 lr: 0.040000
INFO:root:[Epoch 150] train=0.995248 val=0.914200 loss=0.016709 time: 16.694102 lr: 0.004000
INFO:root:[Epoch 151] train=0.996013 val=0.913900 loss=0.014680 time: 16.737249 lr: 0.004000
INFO:root:[Epoch 152] train=0.995892 val=0.914600 loss=0.014513 time: 16.556335 lr: 0.004000
INFO:root:[Epoch 153] train=0.996376 val=0.914900 loss=0.013397 time: 16.645461 lr: 0.004000
INFO:root:[Epoch 154] train=0.996456 val=0.914100 loss=0.013556 time: 16.855890 lr: 0.004000
INFO:root:[Epoch 155] train=0.996617 val=0.915100 loss=0.013111 time: 16.668217 lr: 0.004000
INFO:root:[Epoch 156] train=0.996859 val=0.915000 loss=0.012966 time: 16.964568 lr: 0.004000
INFO:root:[Epoch 157] train=0.996798 val=0.914900 loss=0.013017 time: 16.976524 lr: 0.004000
INFO:root:[Epoch 158] train=0.996859 val=0.915100 loss=0.012436 time: 17.095269 lr: 0.004000
INFO:root:[Epoch 159] train=0.997221 val=0.915400 loss=0.012088 time: 16.504340 lr: 0.004000
INFO:root:[Epoch 160] train=0.997141 val=0.913700 loss=0.012337 time: 16.531899 lr: 0.004000
INFO:root:[Epoch 161] train=0.997241 val=0.914200 loss=0.012035 time: 16.473610 lr: 0.004000
INFO:root:[Epoch 162] train=0.997141 val=0.914900 loss=0.011811 time: 16.616115 lr: 0.004000
INFO:root:[Epoch 163] train=0.997080 val=0.914600 loss=0.011832 time: 16.713447 lr: 0.004000
INFO:root:[Epoch 164] train=0.997161 val=0.914300 loss=0.011593 time: 16.455999 lr: 0.004000
INFO:root:[Epoch 165] train=0.997161 val=0.915000 loss=0.011775 time: 17.037043 lr: 0.004000
INFO:root:[Epoch 166] train=0.996919 val=0.914000 loss=0.011761 time: 16.764914 lr: 0.004000
INFO:root:[Epoch 167] train=0.997101 val=0.916100 loss=0.011923 time: 16.775837 lr: 0.004000
INFO:root:[Epoch 168] train=0.997362 val=0.915100 loss=0.011646 time: 16.645212 lr: 0.004000
INFO:root:[Epoch 169] train=0.997282 val=0.915500 loss=0.011746 time: 16.840034 lr: 0.004000
INFO:root:[Epoch 170] train=0.997825 val=0.914900 loss=0.010228 time: 16.538318 lr: 0.004000
INFO:root:[Epoch 171] train=0.997604 val=0.915400 loss=0.011123 time: 15.645112 lr: 0.004000
INFO:root:[Epoch 172] train=0.997543 val=0.914800 loss=0.010812 time: 15.777306 lr: 0.004000
INFO:root:[Epoch 173] train=0.997523 val=0.914700 loss=0.010995 time: 15.805739 lr: 0.004000
INFO:root:[Epoch 174] train=0.997403 val=0.915300 loss=0.011283 time: 16.499002 lr: 0.004000
INFO:root:[Epoch 175] train=0.997503 val=0.914700 loss=0.010800 time: 17.106987 lr: 0.004000
INFO:root:[Epoch 176] train=0.997382 val=0.915600 loss=0.011188 time: 16.974476 lr: 0.004000
INFO:root:[Epoch 177] train=0.997846 val=0.915600 loss=0.010884 time: 16.204178 lr: 0.004000
INFO:root:[Epoch 178] train=0.997745 val=0.914400 loss=0.010595 time: 16.097060 lr: 0.004000
INFO:root:[Epoch 179] train=0.997684 val=0.914500 loss=0.010511 time: 15.842358 lr: 0.004000
INFO:root:[Epoch 180] train=0.997503 val=0.915200 loss=0.010561 time: 15.938603 lr: 0.004000
INFO:root:[Epoch 181] train=0.997362 val=0.914100 loss=0.010702 time: 15.835923 lr: 0.004000
INFO:root:[Epoch 182] train=0.997423 val=0.914200 loss=0.010778 time: 15.422118 lr: 0.004000
INFO:root:[Epoch 183] train=0.997785 val=0.915100 loss=0.010294 time: 14.633282 lr: 0.004000
INFO:root:[Epoch 184] train=0.997624 val=0.914700 loss=0.010029 time: 14.721981 lr: 0.004000
INFO:root:[Epoch 185] train=0.997624 val=0.915800 loss=0.010735 time: 14.968868 lr: 0.004000
INFO:root:[Epoch 186] train=0.997926 val=0.915000 loss=0.009907 time: 14.547365 lr: 0.004000
INFO:root:[Epoch 187] train=0.998107 val=0.915400 loss=0.010103 time: 14.719040 lr: 0.004000
INFO:root:[Epoch 188] train=0.998087 val=0.915400 loss=0.009745 time: 15.895173 lr: 0.004000
INFO:root:[Epoch 189] train=0.997483 val=0.914500 loss=0.010721 time: 15.383641 lr: 0.004000
INFO:root:[Epoch 190] train=0.997886 val=0.914900 loss=0.009905 time: 14.803699 lr: 0.004000
INFO:root:[Epoch 191] train=0.997986 val=0.915600 loss=0.009980 time: 14.555034 lr: 0.004000
INFO:root:[Epoch 192] train=0.997684 val=0.915100 loss=0.010381 time: 14.385173 lr: 0.004000
INFO:root:[Epoch 193] train=0.997926 val=0.915200 loss=0.009702 time: 14.690795 lr: 0.004000
INFO:root:[Epoch 194] train=0.998067 val=0.916100 loss=0.009993 time: 14.343205 lr: 0.004000
INFO:root:[Epoch 195] train=0.997483 val=0.916900 loss=0.010862 time: 15.455662 lr: 0.004000
INFO:root:[Epoch 196] train=0.997926 val=0.915400 loss=0.009798 time: 14.619699 lr: 0.004000
INFO:root:[Epoch 197] train=0.997906 val=0.916000 loss=0.009652 time: 14.688270 lr: 0.004000
INFO:root:[Epoch 198] train=0.997684 val=0.914700 loss=0.010119 time: 14.890749 lr: 0.004000
INFO:root:[Epoch 199] train=0.997886 val=0.914400 loss=0.009939 time: 14.932174 lr: 0.004000
